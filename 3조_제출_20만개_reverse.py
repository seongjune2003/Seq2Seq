# -*- coding: utf-8 -*-
"""3조_제출_20만개_reverse.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kDOw9CXNPr_Cr-Adqt5UJNOlJBliJWkt

# 목차

1. 학습에 사용한 데이터 - 구어체1, 구어체2
2. 사용한 모델 - Seq2Seq attention
3. 전처리 과정
	* 1) 형태소 분석기 한글 - okt / 영어 -  nltk
	* 2) 영어 reverse 처리
	* 3) 40만 데이터 셋 단어집합 축소
4. 결과
"""

import re
import os
import unicodedata
import urllib3
import zipfile
import shutil
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import Embedding, GRU, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""## Download the data

**사용데이터**

* **구어체1**

* **구어체1 + 구어체2**

**사용한 모델**
* **Seq2Seq attention**
"""

!gdown --id 1V6HsBoEczDoo4NDZ1I5iXSfRxFxCatis  # 데이터 더 필요하면 aihub 여기 데이터에서 문어체같은거 추가해도 됨.

import pandas as pd

df = pd.read_excel('/content/1_구어체(1).xlsx')
df

"""## Modeling

"""

# 직접 만들어보세요!

df.info()

df['원문'].nunique()

df['번역문'].nunique()

hangeul = []
english = []
for idx in df.index:
  hangeul.append(df.loc[idx, '원문'])
  english.append(df.loc[idx, '번역문'])
print(len(hangeul))
print(len(english))

# hangeul

# english

"""Configuration

데이터 20만개 사용
"""

batch_size = 64  # Batch size for training.
epochs = 100  # Number of epochs to train for.
hidden_units = 256  # Latent dimensionality of the encoding space. 백터 크기 
embedding_dim = 64              # 수치화한 값 종류가 64개까지
num_samples = 200000  # Number of samples to train on.    # 학습시킬 데이터 양  # 원래 33000개에서 수정해본다.

"""전처리"""

!pip install konlpy

"""**한글 전처리에 okt 사용해서 형태소 형식으로 처리**"""

from konlpy.tag import Okt

# 한글 전처리 함수
def preprocess_sentence_kor(sent):
  okt = Okt()
  sent = re.sub(r"([?.!,¿])", r" \1", sent)
  sent = re.sub(r"[^a-zA-Z!.?0-9,가-힣]+", r" ", sent)
  sent = re.sub(r"\s+", " ", sent)
  sent = okt.morphs(sent, stem=False)
  return sent

"""영어 전처리
1. nltk 토큰화를 사용  
=> " ".join(list)로 리스트를 다시 문자열로 변환
2. Source_sentence를 reverse 처리.


"""

from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

# 영어 전처리 함수
def preprocess_sentence(sent):
  # 영어 소문자화
  # 단어와 구두점 사이에 공백을 만듭니다.
  # Ex) "he is a boy." => "he is a boy ."
  sent = re.sub(r"([?.!,¿])", r" \1", sent.lower())

  # (a-z, A-Z, ".", "?", "!", ",") 이들을 제외하고는 전부 공백으로 변환합니다.
  sent = re.sub(r"[^a-zA-Z!.?0-9가-힣]+", r" ", sent)   # r  replace   \s 공백  re.sub 제거함수    # 영한에선 맨위거 빼고 호출하면 됨,

  # 다수 개의 공백을 하나의 공백으로 치환
  sent = re.sub(r"\s+", " ", sent)
  sent = word_tokenize(sent)
  return ' '.join(sent)[::-1]     ##리스트에 있는걸 문장으로 바꿔주는게 ' '.join문  ## 영어 문장 reverse

def load_preprocessed_data():
  encoder_input, decoder_input, decoder_target = [], [], []   # 인코더 인풋, 디코더 인풋, 디코더 타겟(정답)   

  for i in range(len(hangeul)):
    # # source 데이터와 target 데이터 분리
    # src_line, tar_line, _ = line.strip().split('\t')

    # source 데이터 전처리    (영어 -> 한글)이니까 영어가 source 한글이 target
    src_line = [w for w in preprocess_sentence(english[i]).split()]

    # target 데이터 전처리
    tar_line_in = preprocess_sentence_kor(hangeul[i])
    tar_line_in.insert(0, "<sos>")
    tar_line_out = preprocess_sentence_kor(hangeul[i])   # 여기 과정을 두 단계로 나눠서 진행해주니 작동한다.
    tar_line_out.append("<eos>")   

    # tar_line_in = tar_line.insert(0, "<sos>")
    # tar_line_out = tar_line.append("<eos>")                                                         # 집가서 이걸로 다시 돌려봐라 
    # tar_line_in = [w for w in ("<sos> " + tar_line).split()]  # teacher forcing을 위한 정답셋     # 새로 만든 전처리함수 하면 리스트형태로 바뀌니까 <sos>를 맨앞자리에 삽입해야됨
    # tar_line_out = [w for w in (tar_line + " <eos>").split()]

    encoder_input.append(src_line)
    decoder_input.append(tar_line_in)  
    decoder_target.append(tar_line_out)

    if i == num_samples - 1:
      break
                
  return encoder_input, decoder_input, decoder_target

# a, b, c = load_preprocessed_data()  # 이건 시험해볼라고 그냥 뽑은거

# a[10]

# b[10], c[10]

# 전처리 테스트
en_sent = u"Have you had dinner?"
han_sent = u"저녁밥은   먹었니?"

print('전처리 전 영어 문장 :', en_sent)
print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))
print('전처리 전 한국어 문장 :', han_sent)
print('전처리 후 한국어 문장 :', preprocess_sentence(han_sent))

sents_en_in, sents_han_in, sents_han_out  = load_preprocessed_data()  # 여기에서 받고

"""피클로 저장"""

# import pickle

# with open('sents_en_in.pickle', 'wb') as f:
# 	pickle.dump(sents_en_in, f)
 
# with open('sents_han_in.pickle', 'wb') as f:
# 	pickle.dump(sents_han_in, f)

# with open('sents_han_out.pickle', 'wb') as f:
# 	pickle.dump(sents_han_out, f)

# sents_han_in  # 여기서 아래 오류 원인 발생 None 값으로 뜬다. (위에 함수 수정함.)

print('인코더의 입력 :',sents_en_in[:5])
print('디코더의 입력 :',sents_han_in[:5])
print('디코더의 레이블 :',sents_han_out[:5])

"""단어집합을 생성하겠습니다."""

# 영어

tokenizer_en = Tokenizer(filters="", lower=False)
tokenizer_en.fit_on_texts(sents_en_in)
encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)  # 단어를 숫자로 바꿈(시퀀스화)

encoder_input

# 한국어
tokenizer_han = Tokenizer(filters="", lower=False)
tokenizer_han.fit_on_texts(sents_han_in)   # input output 두개 받아옴
tokenizer_han.fit_on_texts(sents_han_out)

decoder_input = tokenizer_han.texts_to_sequences(sents_han_in)   # 이걸
decoder_target = tokenizer_han.texts_to_sequences(sents_han_out)

decoder_target[-5:]

"""각 문장을 입력층의 최대 길이만큼 패딩(padding)을 수행합니다."""

encoder_input = pad_sequences(encoder_input, padding='post')   # padding='post'는 단어를 앞으로 당기고 단어 뒤에 부족한 만큼 패딩해줌.
decoder_input = pad_sequences(decoder_input, padding='post')
decoder_target = pad_sequences(decoder_target, padding='post')

print('인코더의 입력의 크기(shape) :',encoder_input.shape)
print('디코더의 입력의 크기(shape) :',decoder_input.shape)
print('디코더의 레이블의 크기(shape) :',decoder_input.shape)   # 인코더와 디코더는 timestep 달라도 됨. 디코더 두개는 같아야됨

"""샘플은 총 33,000개 존재하며 영어 문장의 길이는 43, 한국어 문장의 길이는 25입니다. 단어 집합의 크기를 정의합니다."""

print(tokenizer_en.word_index)
print(tokenizer_han.word_index)   # 많이 나온 단어 순서로 나열. 한글은 앞 3개는 제외하고 4번부터 유효함

src_vocab_size = len(tokenizer_en.word_index) + 1
tar_vocab_size = len(tokenizer_han.word_index) + 1

print(f"영어 단어 집합의 크기 : {src_vocab_size}, 한국어 단어 집합의 크기 : {tar_vocab_size}")

"""단어로부터 정수를 얻는 딕셔너리와 정수로부터 단어를 얻는 딕셔너리를 각각 만들어줍니다. 이들은 훈련을 마치고 예측값과 실제값을 비교하는 단계에서 사용합니다."""

src_to_index = tokenizer_en.word_index  # word : idx
index_to_src = tokenizer_en.index_word  # idx : word
tar_to_index = tokenizer_han.word_index  # word : idx
index_to_tar = tokenizer_han.index_word  # idx : word

print(src_to_index)
print(index_to_src)

"""테스트 데이터를 분리하겠습니다. 인코더 input, 디코더 input, 디코더 output이 동일하게 분리되어야 하기 때문에 랜덤한 정수배열을 만들어 직접 나누겠습니다.

우선 랜덤하게 시퀀스를 섞습니다.
"""

indices = np.arange(encoder_input.shape[0])
np.random.shuffle(indices)
print('랜덤 시퀀스 :',indices)

encoder_input = encoder_input[indices]
decoder_input = decoder_input[indices]
decoder_target = decoder_target[indices]

decoder_input[indices[0]], decoder_target[indices[0]]  # decoder_input 맨 앞 값이 sos. 2번째부터 밑에거랑 같아야함.

"""`<sos>` `<eos>` 토큰을 제외한 다른 단어들은 동일한 인덱스를 갖고 있어야 합니다.

10%의 데이터를 test데이터로 분리하겠습니다.
"""

n_of_val = int(num_samples*0.1)

encoder_input_train = encoder_input[:-n_of_val]  # 뒤에 10%를 자르겠다.
decoder_input_train = decoder_input[:-n_of_val]
decoder_target_train = decoder_target[:-n_of_val]

encoder_input_test = encoder_input[-n_of_val:]
decoder_input_test = decoder_input[-n_of_val:]
decoder_target_test = decoder_target[-n_of_val:]

print('훈련 source 데이터의 크기 :',encoder_input_train.shape)
print('훈련 target 데이터의 크기 :',decoder_input_train.shape)
print('훈련 target 레이블의 크기 :',decoder_target_train.shape)
print('테스트 source 데이터의 크기 :',encoder_input_test.shape)
print('테스트 target 데이터의 크기 :',decoder_input_test.shape)
print('테스트 target 레이블의 크기 :',decoder_target_test.shape)

"""이제 번역기를 빌드해보겠습니다."""

from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking
from tensorflow.keras.models import Model

"""인코더를 설계합니다. LSTM에서 state_h, state_c를 리턴받는데, 이는 각각 RNN 챕터에서 LSTM을 처음 설명할 때 언급하였던 은닉 상태와 셀 상태에 해당됩니다. 이 두 가지 상태를 encoder_states에 저장합니다. encoder_states를 디코더에 전달하므로서 이 두 가지 상태 모두를 디코더로 전달할 예정입니다. 이것이 앞서 배운 컨텍스트 벡터입니다."""

from tensorflow.keras.layers import Input     # 인풋, 임베딩, LSTM
# 인코더
encoder_inputs = Input(shape=(None, ))

# 임베딩 층
enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs)  # 인코더 - 임베딩 연결

# 상태값 리턴을 위해 return_state는 True
encoder_lstm = LSTM(hidden_units, return_sequences = True, return_state = True)  # LSTM 해줌.  

# 은닉 상태와 셀 상태를 리턴
encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)   # 임베딩 연결

encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장

"""이번에는 Attention을 추가합니다. 
- Attention 중 가장 기초적인 dot-product attention을 예제로 진행하겠습니다. 
1. attention score 계산
2. softmax로 attention distribution 계산
3. 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값 계산
4. 어텐션값과 디코더의 t시점의 은닉상태를 연결
5. 출력층 연산의 입력이 되는 st 계산

디코더에서는 attention layer를 추가합니다. 
- 이때, s_는 은닉상태와 디코더의 최종 출력은 연결해야 하므로, shape을 맞춰주기 위하여 축을 추가합니다. 
- attention layer는 디코더의 은닉상태와 인코더의 은닉상태 전체를 받아 컨텍스트 벡터를 생성합니다. 
- 마지막으로 생성한 컨텍스트 벡터와 디코더의 은닉상태 전체를 softmax layer에 넣고 인덱스를 예측합니다.

디코더는 인코더의 마지막 은닉 상태로부터 초기 은닉 상태를 얻습니다. initial_state의 인자값으로 encoder_states를 주는 코드가 이에 해당됩니다. 디코더도 은닉 상태, 셀 상태를 리턴하기는 하지만 훈련 과정에서는 사용하지 않습니다. seq2seq의 디코더는 기본적으로 각 시점마다 다중 클래스 분류 문제를 풀고있습니다. 매 시점마다 프랑스어 단어 집합의 크기(tar_vocab_size)의 선택지에서 단어를 1개 선택하여 이를 이번 시점에서 예측한 단어로 택합니다. 다중 클래스 분류 문제이므로 출력층으로 소프트맥스 함수와 손실 함수를 크로스 엔트로피 함수를 사용합니다.

categorical_crossentropy를 사용하려면 레이블은 원-핫 인코딩이 된 상태여야 합니다. 그런데 현재 decoder_outputs의 경우에는 원-핫 인코딩을 하지 않은 상태입니다. 원-핫 인코딩을 하지 않은 상태로 정수 레이블에 대해서 다중 클래스 분류 문제를 풀고자 하는 경우에는 categorical_crossentropy가 아니라 sparse_categorical_crossentropy를 사용하면 됩니다.
"""

from keras.layers import AdditiveAttention 

class BahdanauAttention(tf.keras.layers.Layer):
  def __init__(self, units):
      super().__init__()

      self.units = units
      self.w1 = tf.keras.layers.Dense(units, use_bias = False)
      self.w2 = tf.keras.layers.Dense(units, use_bias = False)

      self.attention = tf.keras.layers.AdditiveAttention()

  def call(self, query, value):
      
      # w1, ht 구현
      w1_query = self.w1(query)
      # w2, hs 구현
      w2_key = self.w2(value)

      # attention 구현
      context_vector, attention_weights = self.attention(inputs = [w1_query, value, w2_key], return_attention_scores = True)

      return context_vector, attention_weights

  def get_config(self):
      config = super().get_config().copy()
      config.update({
          'units': self.units 
      })
      return config

# 디코더    # 50p 책 그림 seq2seq구조 오른쪽
decoder_inputs = Input(shape=(None, ))   # 디코더 인풋 선언

# 임베딩 층
dec_emb_layer = Embedding(tar_vocab_size, hidden_units)  # 임베딩 layer 만들고

# 임베딩 결과
dec_emb = dec_emb_layer(decoder_inputs)  # 디코더 인풋 받아옴

# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True
decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state = True)

# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용
decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = encoder_states)  # 임베딩 결과를 넣는다. # 인코더 마지막값을 디코더 입력값에 같이 넣어라 


## attention   # 쿼리, key, value 세개 넣어줌.
S_ = tf.concat([state_h[:, tf.newaxis, :], decoder_outputs[:, :-1, :]], axis=1)   # 인코더 마지막 state랑 decorder output 합침. 이게 attention의 쿼리가 됨. 둘의 차원을 맞춰줌.

attention = BahdanauAttention(hidden_units)   
context_vector, _ = attention(S_, encoder_outputs)  # 여기에 key랑 value값이 포함됨.

concat = tf.concat([decoder_outputs, context_vector], axis=-1)   # 결과랑 context_vector랑 concat해서 가져옴. (decoder outputs + query + key + value)


# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측
decoder_dense = Dense(tar_vocab_size, activation = 'softmax')
decoder_outputs = decoder_dense(concat)

# 모델의 입력과 출력을 정의.
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])

model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, 
          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),
          batch_size=128, epochs=10)

model.summary()

tf.keras.utils.plot_model(model, to_file='model.png', )

from keras.models import load_model

model.save('/content/attention.h5')



"""훈련된 모델로 번역하기

모델링
"""

# 인코더 - 훈련과 동일 
encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])    # []  전시점 인코더 아웃풋이랑 마지막 스테이트 가져와라

# 디코더

# 이전 시점의 상태를 보관할 텐서
decoder_state_input_h = Input(shape=(hidden_units,))
decoder_state_input_c = Input(shape=(hidden_units,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]   # 여기서 결과 나온걸

######################
encoder_state_h = Input(shape=(hidden_units,))
encoder_outputs2 = Input(shape =(None, hidden_units,))
######################

# 훈련 때 사용했던 임베딩 층을 재사용
dec_emb2 = dec_emb_layer(decoder_inputs)

# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용
decoder_outputs2, state_h, state_c = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)   # 여기서 받아쓰고
decoder_states2 = [state_h, state_c]

###################### 이게 쿼리
S_ = tf.concat([encoder_state_h[:, tf.newaxis, :], decoder_outputs2[:, :-1, :]], axis=1)

# Attention
context_vector, att_score = attention(S_, encoder_outputs2)
decoder_concat = tf.concat([decoder_outputs2, context_vector], axis=-1)

# 모든 시점에 대해서 단어 예측 (Fully Connected)
decoder_outputs2 = decoder_dense(decoder_concat)   # 얘가 마지막 결과값

# 수정된 디코더
decoder_model = Model(
    [decoder_inputs, encoder_state_h, encoder_outputs2] + decoder_states_inputs,
    [decoder_outputs2, att_score] + decoder_states2)

"""테스트 단계에서의 동작을 위한 decode_sequence 함수를 구현합니다. 
1. 입력 문장이 들어오면 인코더는 마지막 시점까지 전개하여 마지막 시점의 은닉 상태와 셀 상태를 리턴합니다. -> `states_value` 
2. 디코더의 초기 입력으로 <SOS>를 준비합니다. -> `target_seq`
3. 이 두 가지 입력을 가지고 while문 안으로 진입하여 이 두 가지를 디코더의 입력으로 사용합니다. 
4. 이제 디코더는 현재 시점에 대해서 예측합니다. 
    - 예측 벡터로부터 현재 시점의 예측 단어인 target_seq를 얻고, h와 c 이 두 개의 값은 states_value에 저장합니다. 
    - while문의 다음 루프. 즉, 두번째 시점의 디코더의 입력으로 다시 target_seq와 states_value를 사용합니다. 
    - 이를 현재 시점의 예측 단어로 <eos>를 예측하거나 번역 문장의 길이가 50이 넘는 순간까지 반복합니다. 
5. 각 시점마다 번역된 다어는 decoded_sentence에 누적하여 저장하였다가 최종 번역 시퀀스로 리턴합니다.

함수정의
"""

def decode_sequence(input_seq):
  # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음
  encoder_test, states_value = encoder_model.predict(input_seq)


  # <SOS>에 해당하는 정수 생성
  target_seq = np.zeros((1,1))
  target_seq[0, 0] = tar_to_index['<sos>']

  stop_condition = False
  decoded_sentence = ''

  # stop_condition이 True가 될 때까지 루프 반복
  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.
  while not stop_condition:
    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용
    output_tokens, att_score, h, c = decoder_model.predict([target_seq, states_value[0], encoder_test] + states_value)

    # 예측 결과를 단어로 변환
    sampled_token_index = np.argmax(output_tokens[0, -1, :])
    sampled_char = index_to_tar[sampled_token_index]

    # 현재 시점의 예측 단어를 예측 문장에 추가
    decoded_sentence += ' '+sampled_char

    # <eos>에 도달하거나 정해진 길이를 넘으면 중단.
    if (sampled_char == '<eos>' or
        len(decoded_sentence) > 50):
        stop_condition = True

    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장
    target_seq = np.zeros((1,1))
    target_seq[0, 0] = sampled_token_index

    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장
    states_value = [h, c]

  return decoded_sentence

# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환
def seq_to_src(input_seq):
  sentence = ''
  for encoded_word in input_seq:
    if(encoded_word != 0):
      sentence = sentence + index_to_src[encoded_word] + ' '
  return sentence

# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환
def seq_to_tar(input_seq):
  sentence = ''
  for encoded_word in input_seq:
    if(encoded_word != 0 and encoded_word != tar_to_index['<eos>']):
    # if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):
      sentence = sentence + index_to_tar[encoded_word] + ' '
  return sentence

"""샘플 데이터 테스트(훈련데이터, 테스트데이터)"""

for seq_index in [3, 50, 100, 300, 1001]:
  input_seq = encoder_input_train[seq_index: seq_index + 1]
  decoded_sentence = decode_sequence(input_seq)

  print("입력문장 :",seq_to_src(encoder_input_train[seq_index])[::-1])
  print("정답문장 :",seq_to_tar(decoder_input_train[seq_index]))
  print("번역문장 :",decoded_sentence[1:-5])
  print("-"*50)

for seq_index in [3, 50, 100, 300, 1001]:
  input_seq = encoder_input_test[seq_index: seq_index + 1]  # 그냥 [seq_index]로 주면 되는데 굳이 [seq_index: seq_index+1]로 준 이유는 뭘까?? -> shape를 input 형태에 맞춰주기 위해서
  decoded_sentence = decode_sequence(input_seq)

  print("입력문장 :",seq_to_src(encoder_input_test[seq_index])[::-1]) # 여기서 뒤집
  print("정답문장 :",seq_to_tar(decoder_input_test[seq_index]))
  print("번역문장 :",decoded_sentence[1:-5])
  print("-"*50)

from tqdm.notebook import tqdm

input_sentence = []
label_sentence = []
pred_sentence = []

for seq_index in tqdm(range(500)):
  input_seq = encoder_input_test[seq_index: seq_index +1]
  decoded_sentence = decode_sequence(input_seq)
  input_sentence.append(seq_to_src(encoder_input_test[seq_index])[::-1])
  label_sentence.append(seq_to_tar(decoder_input_test[seq_index]))
  pred_sentence.append(decoded_sentence[1:-5])

df_test = pd.DataFrame({'원본' : input_sentence, '정답' : label_sentence, '번역' : pred_sentence})
df_test.head()

df_test.to_csv('/content/attention.csv')

















"""## BLEU Score (Bilingual Evaluation Understudy Score)
기계 번역의 성능이 얼마나 뛰어난가를 측정하기 위해 사용되는 대표적인 방법 중 하나인 BLEU(Bilingual Evaluation Understudy)를 측정해봅시다. 
- 논문 : BLEU: a Method for Automatic Evaluation of Machine Translation
- BLEU 점수는 기계 번역된 텍스트와 고품질 참조 번역 세트의 유사성을 측정하는 0과 1 사이의 숫자입니다. 
    - 값이 0이면 기계 번역된 출력이 참조 번역과 겹치는 부분이 없는 것을 의미하고(저품질)
    - 1이면 참조 번역과 완벽하게 겹치는 것을 의미합니다(고품질)

- BLEU 설명 
    - https://jrc-park.tistory.com/273
    - https://wikidocs.net/31695
    - https://cloud.google.com/translate/automl/docs/evaluate?hl=ko#bleu

BLEU는 기계 번역 결과와 사람이 직접 번역한 결과가 얼마나 유사한지 비교하여 번역에 대한 성능을 측정하는 방법입니다.
"""

import numpy as np
from collections import Counter
from nltk import ngrams

import nltk.translate.bleu_score as bleu

candidate = 'It is a guide to action which ensures that the military always obeys the commands of the party'
references = [
    'It is a guide to action that ensures that the military will forever heed Party commands',
    'It is the guiding principle which guarantees the military forces always being under the command of the Party',
    'It is the practical guide for the army always to heed the directions of the party'
]

print('패키지 NLTK의 BLEU :',bleu.sentence_bleu(list(map(lambda ref: ref.split(), references)),candidate.split()))

ref_train = [seq_to_tar(sen) for sen in decoder_input_train[:100]]

from tqdm.notebook import tqdm 

pred_train = []
for idx in tqdm(range(len(encoder_input_train[:100]))):
    pred_train.append(decode_sequence(encoder_input_train[idx:idx+1]))

bleu_score = 0
for ref, pred in zip(ref_train, pred_train):
    if len(ref) == 0 or len(pred) == 0:
        continue
    bleu_score += bleu.sentence_bleu(ref, pred)

bleu_score = bleu_score/len(ref_train)
bleu_score

ref_test = [seq_to_tar(sen) for sen in decoder_input_test[:100]]
pred_test = [decode_sequence(encoder_input_test[idx:idx+1]) for idx in tqdm(range(len(encoder_input_test[:100])))]

bleu_score = 0
for ref, pred in zip(ref_test, pred_test):
    if len(ref) == 0 or len(pred) == 0:
        continue
    bleu_score += bleu.sentence_bleu(ref, pred)

bleu_score = bleu_score/len(ref_test)
bleu_score

"""### Reference

- https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html
- https://wikidocs.net/86900
"""

df_test = pd.read_csv('/content/영한테스트_영어.csv')
df_test

decoded_sent = []

for idx, row in df_test.iterrows():
    src_line = preprocess_sentence(row[0])  # 문장들 영어 전처리
    src_line = [w for w in src_line.split()]  # 문장 단어로 split
    decoded_sent.append(src_line)              # decorded_sent에 추가

decoded_sent[:5]

len(decoded_sent)

decoded_sent[0]

# 영어
encoder_input_t = tokenizer_en.texts_to_sequences(decoded_sent)  # 토큰화 + 단어를 숫자로 바꿈(시퀀스화)

encoder_input_t = pad_sequences(encoder_input_t, padding='post')  # 패딩처리

encoder_input_t[:5]

answer = []
for seq_index in range(500):
  input_seq = encoder_input_t[seq_index: seq_index + 1] 
  decoded_sentence = decode_sequence(input_seq)
  answer.append(decoded_sentence[1:-5])

len(answer)

df2 = pd.DataFrame(answer)

df2

df = pd.concat([df_test, df2], axis = 1)
df

df = df.rename(columns={0:'번역'})
df

df.to_csv('3조_제출용.csv')

